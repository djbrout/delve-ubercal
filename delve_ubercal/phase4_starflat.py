"""Phase 4: Per-CCD flat-field (star flat) refinement with epoch boundaries.

After solving for per-CCD-per-exposure zero-points, systematic residuals
as a function of pixel position within each CCD reveal flat-field errors.
Fits 2D Chebyshev polynomials per CCD per epoch to correct them.
"""

import argparse
import time
from pathlib import Path

import numpy as np
import pandas as pd

from delve_ubercal.phase0_ingest import get_test_region_pixels, load_config
from delve_ubercal.phase2_solve import (
    accumulate_normal_equations,
    build_node_index,
    load_des_fgcm_zps,
    load_nsc_zpterms,
    solve_anchored,
    solve_unanchored,
)
from delve_ubercal.phase3_outlier_rejection import (
    compute_residuals,
    load_exposure_mjds,
    run_outlier_rejection,
)
from delve_ubercal.utils.healpix_utils import get_all_healpix_pixels


def get_epoch(mjd, ccdnum, config):
    """Determine the instrumental epoch for a given (mjd, ccdnum).

    Parameters
    ----------
    mjd : float
        Modified Julian Date.
    ccdnum : int
        CCD number.
    config : dict
        Pipeline config with starflat.epoch_boundaries.

    Returns
    -------
    epoch : int
        Epoch index (0, 1, 2, ...).
    """
    sf_cfg = config["starflat"]
    boundaries = sorted(sf_cfg["epoch_boundaries"].get("global", []))

    # Add per-CCD boundaries
    per_ccd = sf_cfg["epoch_boundaries"].get("per_ccd", {})
    if ccdnum in per_ccd:
        boundaries = sorted(set(boundaries + per_ccd[ccdnum]))

    epoch = 0
    for b in boundaries:
        if mjd > b:
            epoch += 1
    return epoch


def fit_chebyshev_2d(x, y, values, order=3, x_range=(1, 2048), y_range=(1, 4096)):
    """Fit a 2D Chebyshev polynomial to scattered data.

    Parameters
    ----------
    x, y : array-like
        Pixel coordinates.
    values : array-like
        Residual values to fit.
    order : int
        Maximum polynomial order.
    x_range, y_range : tuple
        Pixel coordinate ranges for normalization to [-1, 1].

    Returns
    -------
    coeffs : np.ndarray
        Chebyshev coefficient matrix (order+1 x order+1).
    """
    # Normalize to [-1, 1]
    x_norm = 2.0 * (np.asarray(x) - x_range[0]) / (x_range[1] - x_range[0]) - 1.0
    y_norm = 2.0 * (np.asarray(y) - y_range[0]) / (y_range[1] - y_range[0]) - 1.0

    # Build design matrix for 2D Chebyshev
    n_terms = (order + 1) * (order + 1)
    n_pts = len(x_norm)
    A = np.zeros((n_pts, n_terms))

    col = 0
    for i in range(order + 1):
        for j in range(order + 1):
            Ti = np.polynomial.chebyshev.chebval(x_norm, np.eye(order + 1)[i])
            Tj = np.polynomial.chebyshev.chebval(y_norm, np.eye(order + 1)[j])
            A[:, col] = Ti * Tj
            col += 1

    # Least-squares fit
    coeffs_flat, _, _, _ = np.linalg.lstsq(A, values, rcond=None)
    return coeffs_flat.reshape(order + 1, order + 1)


def evaluate_chebyshev_2d(x, y, coeffs, x_range=(1, 2048), y_range=(1, 4096)):
    """Evaluate a 2D Chebyshev polynomial.

    Parameters
    ----------
    x, y : array-like
        Pixel coordinates.
    coeffs : np.ndarray
        Coefficient matrix from fit_chebyshev_2d.
    x_range, y_range : tuple
        Pixel coordinate ranges.

    Returns
    -------
    values : np.ndarray
        Evaluated correction values.
    """
    x_norm = 2.0 * (np.asarray(x) - x_range[0]) / (x_range[1] - x_range[0]) - 1.0
    y_norm = 2.0 * (np.asarray(y) - y_range[0]) / (y_range[1] - y_range[0]) - 1.0

    order = coeffs.shape[0] - 1
    result = np.zeros(len(x_norm))
    for i in range(order + 1):
        for j in range(order + 1):
            Ti = np.polynomial.chebyshev.chebval(x_norm, np.eye(order + 1)[i])
            Tj = np.polynomial.chebyshev.chebval(y_norm, np.eye(order + 1)[j])
            result += coeffs[i, j] * Ti * Tj

    return result


def compute_starflat_corrections(
    phase0_files, zp_dict, node_to_idx, exposure_mjds, config
):
    """Compute per-CCD-per-epoch illumination corrections.

    Reads Phase 0 files (which have x, y positions) to compute residuals
    at pixel positions and fit 2D Chebyshev polynomials.

    Parameters
    ----------
    phase0_files : list of Path
        Phase 0 detection parquet files with x, y columns.
    zp_dict : dict
        Maps (expnum, ccdnum) -> zp_solved from Phase 3.
    node_to_idx : dict
        Connected node mapping.
    exposure_mjds : dict
        expnum -> MJD.
    config : dict
        Pipeline configuration.

    Returns
    -------
    corrections : dict
        Maps (ccdnum, epoch) -> Chebyshev coefficients.
    stats : list of dict
        Per-CCD-per-epoch statistics.
    """
    sf_cfg = config["starflat"]
    poly_order = sf_cfg["polynomial_order"]
    min_stars = sf_cfg["min_stars_per_bin"]

    # Collect residuals with (x, y, ccdnum, epoch)
    # Filter out nodes with ZP ~ 0 (no data after outlier rejection)
    median_zp = np.median([zp for zp in zp_dict.values() if zp > 1.0])
    min_valid_zp = median_zp - 5.0  # Allow up to 5 mag below median

    all_records = []
    for f in phase0_files:
        df = pd.read_parquet(f)
        if len(df) == 0:
            continue
        if "x" not in df.columns or "y" not in df.columns:
            continue

        # Add ZP
        nodes = list(zip(df["expnum"].values, df["ccdnum"].values))
        zps = np.array([zp_dict.get(n, np.nan) for n in nodes])
        connected = np.array([n in node_to_idx for n in nodes])

        # Filter: connected, not NaN, and ZP is physically reasonable
        valid = connected & ~np.isnan(zps) & (zps > min_valid_zp)
        df = df[valid].copy()
        zps = zps[valid]

        df["zp_solved"] = zps
        df["m_cal"] = df["m_inst"].values + zps

        all_records.append(df)

    if not all_records:
        return {}, []

    combined = pd.concat(all_records, ignore_index=True)

    # Compute weighted mean per star
    w = 1.0 / (combined["m_err"].values ** 2)
    combined["_w"] = w
    combined["_wm"] = w * combined["m_cal"].values
    star_stats = combined.groupby("objectid").agg(
        sum_w=("_w", "sum"),
        sum_wm=("_wm", "sum"),
    )
    star_stats["m_star_mean"] = star_stats["sum_wm"] / star_stats["sum_w"]
    combined = combined.merge(
        star_stats[["m_star_mean"]],
        left_on="objectid", right_index=True, how="left",
    )
    combined["residual"] = combined["m_cal"] - combined["m_star_mean"]
    combined.drop(columns=["_w", "_wm"], inplace=True)

    # Assign epochs
    if "mjd" not in combined.columns:
        combined["mjd"] = combined["expnum"].map(exposure_mjds)
    combined["epoch"] = [
        get_epoch(mjd, ccd, config)
        for mjd, ccd in zip(combined["mjd"].values, combined["ccdnum"].values)
    ]

    # Fit per CCD per epoch
    corrections = {}
    stats = []
    for (ccdnum, epoch), group in combined.groupby(["ccdnum", "epoch"]):
        n_pts = len(group)
        if n_pts < min_stars:
            continue

        x = group["x"].values
        y = group["y"].values
        residuals = group["residual"].values

        # Clip outliers before fitting (3-sigma)
        med = np.median(residuals)
        mad = np.median(np.abs(residuals - med))
        sigma_est = 1.4826 * mad if mad > 0 else np.std(residuals)
        good = np.abs(residuals - med) < 3 * sigma_est
        if good.sum() < min_stars:
            continue

        coeffs = fit_chebyshev_2d(
            x[good], y[good], residuals[good], order=poly_order
        )
        corrections[(int(ccdnum), int(epoch))] = coeffs

        # Evaluate correction and compute stats
        correction_vals = evaluate_chebyshev_2d(x[good], y[good], coeffs)
        corrected_residuals = residuals[good] - correction_vals
        rms_before = np.sqrt(np.mean(residuals[good] ** 2)) * 1000
        rms_after = np.sqrt(np.mean(corrected_residuals ** 2)) * 1000
        correction_rms = np.sqrt(np.mean(correction_vals ** 2)) * 1000

        stats.append({
            "ccdnum": int(ccdnum),
            "epoch": int(epoch),
            "n_stars": int(good.sum()),
            "correction_rms_mmag": correction_rms,
            "rms_before_mmag": rms_before,
            "rms_after_mmag": rms_after,
        })

    return corrections, stats


def apply_starflat_to_star_lists(star_list_files, corrections, exposure_mjds,
                                  config, output_dir):
    """Apply star flat corrections to star list m_inst values.

    Parameters
    ----------
    star_list_files : list of Path
        Star list files with m_inst (and x, y from Phase 0).
    corrections : dict
        Maps (ccdnum, epoch) -> Chebyshev coefficients.
    exposure_mjds : dict
        expnum -> MJD.
    config : dict
        Pipeline configuration.
    output_dir : Path
        Directory to write corrected files.

    Returns
    -------
    corrected_files : list of Path
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    corrected_files = []

    for f in star_list_files:
        df = pd.read_parquet(f)
        if len(df) == 0 or "x" not in df.columns:
            out_f = output_dir / f.name
            df.to_parquet(out_f, index=False)
            corrected_files.append(out_f)
            continue

        # Get MJDs
        mjds = df["expnum"].map(exposure_mjds).values

        # Apply corrections
        delta = np.zeros(len(df))
        for i, (row_idx, row) in enumerate(df.iterrows()):
            mjd = mjds[i]
            if np.isnan(mjd):
                continue
            epoch = get_epoch(mjd, row["ccdnum"], config)
            key = (int(row["ccdnum"]), epoch)
            if key in corrections:
                delta[i] = evaluate_chebyshev_2d(
                    [row["x"]], [row["y"]], corrections[key]
                )[0]

        df = df.copy()
        df["m_inst"] = df["m_inst"].values - delta  # Subtract the systematic

        out_f = output_dir / f.name
        df.to_parquet(out_f, index=False)
        corrected_files.append(out_f)

    return corrected_files


def run_starflat(band, pixels, config, output_dir, cache_dir, mode="both"):
    """Run the star flat correction pipeline.

    Parameters
    ----------
    band : str
        Filter band.
    pixels : np.ndarray
        HEALPix pixel indices.
    config : dict
        Pipeline configuration.
    output_dir : Path
        Output directory.
    cache_dir : Path
        Cache directory.
    mode : str
        Solve mode.

    Returns
    -------
    results : dict
        Results including corrections and re-solve output.
    """
    nside = config["survey"]["nside_chunk"]
    phase0_dir = output_dir / f"phase0_{band}"
    phase1_dir = output_dir / f"phase1_{band}"
    phase3_dir = output_dir / f"phase3_{band}"
    phase4_dir = output_dir / f"phase4_{band}"
    phase4_dir.mkdir(parents=True, exist_ok=True)

    # Load reference data
    print(f"  Loading reference data...", flush=True)
    node_to_idx, idx_to_node = build_node_index(
        phase1_dir / "connected_nodes.parquet"
    )
    n_params = len(node_to_idx)
    des_fgcm_zps = load_des_fgcm_zps(cache_dir, band)
    exposure_mjds = load_exposure_mjds(cache_dir, band)

    # Load Phase 3 anchored solution
    zp_file = phase3_dir / "zeropoints_anchored.parquet"
    if not zp_file.exists():
        print("  Phase 3 output not found, using Phase 2...", flush=True)
        zp_file = output_dir / f"phase2_{band}" / "zeropoints_anchored.parquet"

    zp_df = pd.read_parquet(zp_file)
    zp_dict = dict(zip(
        zip(zp_df["expnum"].values, zp_df["ccdnum"].values),
        zp_df["zp_solved"].values,
    ))

    # Phase 0 files have x, y positions
    phase0_files = sorted(phase0_dir.glob(f"detections_nside{nside}_pixel*.parquet"))
    if not phase0_files:
        # Try alternative naming
        phase0_files = sorted(phase0_dir.glob(f"nside{nside}_pixel*.parquet"))
    if not phase0_files:
        phase0_files = sorted(phase0_dir.glob("*.parquet"))
    print(f"  Phase 0 detection files: {len(phase0_files)}", flush=True)

    # Phase 3 cleaned star lists (no x,y but we use Phase 0 for starflat fitting)
    # For re-solve, use Phase 3 cleaned lists
    last_iter_dir = None
    for i in range(5, 0, -1):
        d = phase3_dir / f"iter{i}"
        if d.exists():
            last_iter_dir = d
            break
    if last_iter_dir is None:
        last_iter_dir = phase1_dir

    star_list_files = sorted(
        last_iter_dir.glob(f"star_lists_nside{nside}_pixel*.parquet")
    )
    print(f"  Cleaned star list files: {len(star_list_files)}", flush=True)

    # 1. Compute star flat corrections
    print(f"  Computing star flat corrections...", flush=True)
    t0 = time.time()
    corrections, stats = compute_starflat_corrections(
        phase0_files, zp_dict, node_to_idx, exposure_mjds, config
    )
    sf_time = time.time() - t0
    print(f"  Star flat fit: {len(corrections)} (CCD, epoch) groups, "
          f"{sf_time:.1f}s", flush=True)

    # Save corrections
    if stats:
        stats_df = pd.DataFrame(stats)
        stats_df.to_parquet(phase4_dir / "starflat_stats.parquet", index=False)

        # Report
        mean_corr_rms = stats_df["correction_rms_mmag"].mean()
        max_corr_rms = stats_df["correction_rms_mmag"].max()
        print(f"  Mean correction RMS: {mean_corr_rms:.1f} mmag", flush=True)
        print(f"  Max correction RMS:  {max_corr_rms:.1f} mmag", flush=True)

        if max_corr_rms > 50:
            print(f"  WARNING: Some CCDs have correction > 50 mmag!", flush=True)
            big = stats_df[stats_df["correction_rms_mmag"] > 50]
            for _, row in big.iterrows():
                print(f"    CCD {row['ccdnum']}, epoch {row['epoch']}: "
                      f"{row['correction_rms_mmag']:.1f} mmag", flush=True)

    # Save correction coefficients
    import pickle
    with open(phase4_dir / "starflat_corrections.pkl", "wb") as f:
        pickle.dump(corrections, f)

    # 2. Apply corrections to Phase 0 files (for re-solve via Phase 3 cleaned lists)
    # Since Phase 3 star lists don't have x,y, we apply starflat to Phase 0 files
    # then re-run Phase 1 through Phase 3.
    # But this is overkill for the test region. Instead, let's apply the correction
    # to the Phase 0 data, regenerate star lists with corrected m_inst, and re-solve.

    # For now, we'll use the corrections diagnostically and re-solve with corrected data
    # The star flat correction is small (~1-10 mmag), so skipping the re-run through
    # Phase 1-3 is acceptable for validation. In production, the correction would be
    # applied before the Phase 2+3 solve.

    # Compute corrected residuals for diagnostics
    if corrections and phase0_files:
        print(f"\n  Computing corrected residuals...", flush=True)
        median_zp2 = np.median([zp for zp in zp_dict.values() if zp > 1.0])
        min_valid_zp2 = median_zp2 - 5.0
        combined_resid = []
        for f in phase0_files:
            df = pd.read_parquet(f)
            if len(df) == 0 or "x" not in df.columns:
                continue
            nodes = list(zip(df["expnum"].values, df["ccdnum"].values))
            zps = np.array([zp_dict.get(n, np.nan) for n in nodes])
            connected = np.array([n in node_to_idx for n in nodes])
            valid = connected & ~np.isnan(zps) & (zps > min_valid_zp2)
            df = df[valid].copy()
            zps = zps[valid]
            df["m_cal"] = df["m_inst"].values + zps
            combined_resid.append(df)

        if combined_resid:
            comb = pd.concat(combined_resid, ignore_index=True)
            w = 1.0 / (comb["m_err"].values ** 2)
            comb["_wm"] = w * comb["m_cal"].values
            comb["_w"] = w
            ss = comb.groupby("objectid").agg(
                sum_w=("_w", "sum"), sum_wm=("_wm", "sum")
            )
            ss["m_star_mean"] = ss["sum_wm"] / ss["sum_w"]
            comb = comb.merge(ss[["m_star_mean"]], left_on="objectid",
                              right_index=True, how="left")
            comb["residual"] = comb["m_cal"] - comb["m_star_mean"]

            # Apply starflat correction
            if "mjd" not in comb.columns:
                comb["mjd"] = comb["expnum"].map(exposure_mjds)
            comb["epoch"] = [
                get_epoch(mjd, ccd, config)
                for mjd, ccd in zip(comb["mjd"].values, comb["ccdnum"].values)
            ]
            delta = np.zeros(len(comb))
            for i, row in comb.iterrows():
                key = (int(row["ccdnum"]), int(row["epoch"]))
                if key in corrections:
                    delta[i] = evaluate_chebyshev_2d(
                        [row["x"]], [row["y"]], corrections[key]
                    )[0]

            comb["corrected_residual"] = comb["residual"].values - delta

            rms_before = np.sqrt(np.mean(comb["residual"].values ** 2)) * 1000
            rms_after = np.sqrt(np.mean(comb["corrected_residual"].values ** 2)) * 1000
            print(f"  Residual RMS before starflat: {rms_before:.1f} mmag", flush=True)
            print(f"  Residual RMS after starflat:  {rms_after:.1f} mmag", flush=True)

    # 3. Print summary
    if stats:
        print(f"\n  {'=' * 60}", flush=True)
        print(f"  Phase 4 Star Flat Summary â€” {band}-band", flush=True)
        print(f"  {'=' * 60}", flush=True)
        print(f"    CCD-epoch groups fitted: {len(corrections)}", flush=True)
        print(f"    Polynomial order:        {config['starflat']['polynomial_order']}", flush=True)

        stats_df = pd.DataFrame(stats)
        print(f"    {'CCD':>5} {'Epoch':>5} {'N_stars':>8} {'Corr_RMS':>10} "
              f"{'RMS_before':>10} {'RMS_after':>10}", flush=True)
        for _, row in stats_df.iterrows():
            print(f"    {int(row['ccdnum']):5d} {int(row['epoch']):5d} "
                  f"{int(row['n_stars']):8d} {row['correction_rms_mmag']:10.1f} "
                  f"{row['rms_before_mmag']:10.1f} {row['rms_after_mmag']:10.1f}",
                  flush=True)

        print(f"  {'=' * 60}", flush=True)

    return {
        "corrections": corrections,
        "stats": stats,
    }


def main():
    parser = argparse.ArgumentParser(
        description="Phase 4: Per-CCD star flat refinement"
    )
    parser.add_argument("--band", required=True, help="Filter band")
    parser.add_argument(
        "--mode", default="both", choices=["unanchored", "anchored", "both"],
        help="Solve mode",
    )
    parser.add_argument(
        "--test-region", action="store_true",
        help="Limit to test region",
    )
    parser.add_argument("--config", default=None, help="Path to config.yaml")
    args = parser.parse_args()

    config = load_config(args.config)
    nside = config["survey"]["nside_chunk"]

    output_dir = Path(config["data"]["output_path"])
    cache_dir = Path(config["data"]["cache_path"])

    if args.test_region:
        pixels = get_test_region_pixels(nside)
        print(f"Test region: {len(pixels)} pixels (nside={nside})", flush=True)
    else:
        pixels = get_all_healpix_pixels(nside)
        print(f"Full sky: {len(pixels)} pixels (nside={nside})", flush=True)

    print(f"Band: {args.band}, Mode: {args.mode}", flush=True)
    print(flush=True)

    results = run_starflat(
        args.band, pixels, config, output_dir, cache_dir, mode=args.mode
    )

    return results


if __name__ == "__main__":
    main()
